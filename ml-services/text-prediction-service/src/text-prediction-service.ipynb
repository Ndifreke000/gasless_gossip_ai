{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T14:04:52.900423Z","iopub.execute_input":"2025-03-21T14:04:52.900694Z","iopub.status.idle":"2025-03-21T14:04:53.331628Z","shell.execute_reply.started":"2025-03-21T14:04:52.900671Z","shell.execute_reply":"2025-03-21T14:04:53.330555Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport string\nfrom transformers import BertTokenizer, BertForMaskedLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T14:11:05.286519Z","iopub.execute_input":"2025-03-21T14:11:05.286859Z","iopub.status.idle":"2025-03-21T14:11:37.273727Z","shell.execute_reply.started":"2025-03-21T14:11:05.286831Z","shell.execute_reply":"2025-03-21T14:11:37.272256Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install fastapi nest-asyncio pyngrok uvicorn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T14:17:16.556586Z","iopub.execute_input":"2025-03-21T14:17:16.557275Z","iopub.status.idle":"2025-03-21T14:17:24.194022Z","shell.execute_reply.started":"2025-03-21T14:17:16.557234Z","shell.execute_reply":"2025-03-21T14:17:24.192580Z"}},"outputs":[{"name":"stdout","text":"Collecting fastapi\n  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\nCollecting pyngrok\n  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\nCollecting uvicorn\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nCollecting starlette<0.47.0,>=0.40.0 (from fastapi)\n  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.11.0a2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.29.0)\nRequirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (3.7.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.2.2)\nDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\nDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: uvicorn, pyngrok, starlette, fastapi\nSuccessfully installed fastapi-0.115.11 pyngrok-7.2.3 starlette-0.46.1 uvicorn-0.34.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"TOP_K = 10\n\nclass Predictor:\n    def __init__(self, model_path = None):\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased').eval()\n\n    def decode(self,tokenizer, pred_idx, top_clean):\n        ignore_tokens = string.punctuation + '[PAD]'\n        tokens = []\n        for w in pred_idx:\n            token = ''.join(tokenizer.decode(w).split())\n            if token not in ignore_tokens:\n                tokens.append(token.replace('##', ''))\n        return '\\n'.join(tokens[:top_clean])\n\n    def encode(self, tokenizer, text_sentence, add_special_tokens=True):\n        text_sentence = text_sentence.replace('<mask>', tokenizer.mask_token)\n        if tokenizer.mask_token == text_sentence.split()[-1]:\n            text_sentence += ' .'\n        input_ids = torch.tensor([tokenizer.encode(text_sentence, add_special_tokens=add_special_tokens)])\n        mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n        return input_ids, mask_idx\n\n    def get_all_predictions(self, text_sentence, top_clean=5):\n        input_ids, mask_idx = self.encode(self.bert_tokenizer, text_sentence)\n        with torch.no_grad():\n            predict = self.bert_model(input_ids)[0]\n        predicted_words = self.decode(self.bert_tokenizer, predict[0, mask_idx, :].topk(TOP_K).indices.tolist(), top_clean)\n        return predicted_words\n\n    def gen_m_words_n_predictions(self, m, n, input_text):\n        output = []\n        res = self.get_all_predictions(input_text + ' <mask>', top_clean=n).split('\\n')\n        input = input_text\n        for i in res:\n            input_text = input+' '+i\n            for i in range(m-1):\n                word = self.get_all_predictions(input_text + ' <mask>', top_clean=1).split('\\n')\n                input_text = input_text+ ' ' + word[0]\n            output.append(input_text)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T14:40:49.163589Z","iopub.execute_input":"2025-03-21T14:40:49.163953Z","iopub.status.idle":"2025-03-21T14:40:49.175725Z","shell.execute_reply.started":"2025-03-21T14:40:49.163924Z","shell.execute_reply":"2025-03-21T14:40:49.174347Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from fastapi import HTTPException\nfrom pydantic import BaseModel\n\n# Create an instance of Predictor\nnextWord = Predictor()\n\n# Define a Pydantic model for input data validation\nclass NextWordInput(BaseModel):\n    text: str\n    predictions: int\n    tokens: int\n\n# GET method to check service status\ndef get_service_status():\n    return {\"status\": \"success\", \"message\": \"Service is running\"}\n\n# POST method to generate next word predictions\ndef get_next_words(data: NextWordInput):\n    try:\n        result = nextWord.gen_m_words_n_predictions(data.tokens, data.predictions, data.text)\n        return {\"status\": \"success\", \"words\": result}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Something went wrong: {type(e).__name__} {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T14:42:10.649030Z","iopub.execute_input":"2025-03-21T14:42:10.649401Z","iopub.status.idle":"2025-03-21T14:42:16.463628Z","shell.execute_reply.started":"2025-03-21T14:42:10.649349Z","shell.execute_reply":"2025-03-21T14:42:16.462343Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cca404ffb5548b49d247f8825e08058"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"053d9c9e43764602a70f14f4aebce218"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f35ce90e443e45e5a97337cf53ed6a45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aef9936562df4ba9a49703adc11c08ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"720a69d65d0b4b06a7ad66048f1a40e9"}},"metadata":{}},{"name":"stderr","text":"BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nimport nest_asyncio\nfrom pyngrok import ngrok\nimport uvicorn\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins = [\"*\"],\n    allow_credentials = True,\n    allow_headers = [\"*\"],\n    allow_methods = [\"*\"],\n)\n\n@app.get(\"/\")\ndef read_root():\n    return get_service_status()\n\n@app.post(\"/predict\")\nasync def post_next_word(data: NextWordInput):\n    return get_next_words(data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T15:49:18.110003Z","iopub.execute_input":"2025-03-21T15:49:18.110411Z","iopub.status.idle":"2025-03-21T15:49:18.117886Z","shell.execute_reply.started":"2025-03-21T15:49:18.110346Z","shell.execute_reply":"2025-03-21T15:49:18.116657Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"!ngrok help","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T15:56:10.806148Z","iopub.execute_input":"2025-03-21T15:56:10.806559Z","iopub.status.idle":"2025-03-21T15:56:11.134428Z","shell.execute_reply.started":"2025-03-21T15:56:10.806527Z","shell.execute_reply":"2025-03-21T15:56:11.132969Z"}},"outputs":[{"name":"stdout","text":"NAME:\n  ngrok - tunnel local ports to public URLs and inspect traffic\n\nUSAGE:\n  ngrok [command] [flags]\n\nDESCRIPTION: \n  ngrok exposes local networked services behinds NATs and firewalls to the\n  public internet over a secure tunnel. Share local websites, build/test\n  webhook consumers and self-host personal services.\n  Detailed help for each command is available by adding '--help' to any command or with\n  the 'ngrok help' command.\n  Open https://dashboard.ngrok.com/obs/traffic-inspector to inspect traffic.\n\n\nTERMS OF SERVICE: https://ngrok.com/tos\n\nEXAMPLES: \n  ngrok http 80                           # secure public URL for port 80 web server\n  ngrok http --url baz.ngrok.dev 8080     # port 8080 available at baz.ngrok.dev\n  ngrok http foo.dev:80                   # tunnel to host:port instead of localhost\n  ngrok http https://localhost            # expose a local https server\n  ngrok tcp 22                            # tunnel arbitrary TCP traffic to port 22\n  ngrok tls --url=foo.com 443             # TLS traffic for foo.com to port 443\n  ngrok start foo bar baz                 # start tunnels from the configuration file\n\nCOMMANDS:\n  api                            use ngrok agent as an api client\n  completion                     generates shell completion code for bash or zsh\n  config                         update or migrate ngrok's configuration file\n  credits                        prints author and licensing information\n  diagnose                       diagnose connection issues\n  help                           Help about any command\n  http                           start an HTTP tunnel\n  service                        run and control an ngrok service on a target operating system\n  start                          start endpoints or tunnels by name from the configuration file\n  tcp                            start a TCP tunnel\n  tls                            start a TLS tunnel\n  tunnel                         start a tunnel for use with a tunnel-group backend\n  update                         update ngrok to the latest version\n  version                        print the version string\n\nOPTIONS:\n      --config strings    path to config files; they are merged if multiple\n  -h, --help              help for ngrok\n      --metadata string   opaque user-defined metadata for the tunnel session\n  -v, --version           version for ngrok\n\nPYNGROK VERSION:\n   7.2.3\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!ngrok config add-authtoken 2Bc5ezPv3N4qiggg9ZMSsP0oS21_bbJAVhavuUapKCjRubtz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T15:57:30.674659Z","iopub.execute_input":"2025-03-21T15:57:30.675152Z","iopub.status.idle":"2025-03-21T15:57:31.007497Z","shell.execute_reply.started":"2025-03-21T15:57:30.675111Z","shell.execute_reply":"2025-03-21T15:57:31.005873Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"port = 8000\nngrok_tunnel = ngrok.connect(port)\nprint('Public URL:', ngrok_tunnel.public_url)\n\nnest_asyncio.apply()\n\nuvicorn.run(app, port=port)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T15:57:35.644390Z","iopub.execute_input":"2025-03-21T15:57:35.644790Z","iopub.status.idle":"2025-03-21T16:49:34.799501Z","shell.execute_reply.started":"2025-03-21T15:57:35.644759Z","shell.execute_reply":"2025-03-21T16:49:34.798320Z"}},"outputs":[{"name":"stdout","text":"Public URL: https://8cb5-34-32-170-53.ngrok-free.app\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [31]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n","output_type":"stream"},{"name":"stdout","text":"INFO:     105.115.0.175:0 - \"POST /predict HTTP/1.1\" 422 Unprocessable Entity\nINFO:     105.115.0.175:0 - \"POST /predict HTTP/1.1\" 422 Unprocessable Entity\nINFO:     105.115.0.175:0 - \"POST /predict HTTP/1.1\" 200 OK\nINFO:     105.115.0.175:0 - \"POST /predict HTTP/1.1\" 200 OK\nINFO:     105.115.0.175:0 - \"POST /predict HTTP/1.1\" 200 OK\nINFO:     105.115.0.175:0 - \"POST /predict HTTP/1.1\" 200 OK\nINFO:     105.115.0.175:0 - \"POST /predict HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Shutting down\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [31]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}